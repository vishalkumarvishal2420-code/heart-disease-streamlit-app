# -*- coding: utf-8 -*-
"""HeartDisease.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iSnnnSWeaUYxBkGX8azv7KR59Qiq3qkn
"""

#Basices libraries import
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

#read heart disease dataset
df = pd.read_csv('/content/heart.csv')

# Basics understanding dataset
print(df.head(5))

# check null value
print(df.isnull().sum())

# check duplicate value
print(df.duplicated().sum())

# check info
print(df.info())

# check numerical distribution
print(df.describe())
# restingBp and Cholesterol has some wrong values, it is not possible a live person has 0 RestingBP and 0 Cholesterol i will have to fix this

df.sample(5)

"""##EDA Univariate and Bivariate Analysis and data preprocessing"""

# Age
# Age is normal distrubuted no outliers and no missing values
sns.histplot(df['Age']) # Histplot for distribution

sns.displot(df['Age'],kde = True)

df['Age'].plot(kind='box')

"""#Age and HeartDisease->
 if age greater then 50 heart Diesease high chance   

"""

sns.barplot(x= df['HeartDisease'],y=df['Age'])

sns.boxplot(x=df['HeartDisease'],y=df['Age'])

sns.violinplot(x=df['HeartDisease'],y=df['Age'])

df[df['HeartDisease']==0]['Age'].plot(kind='kde')
df[df['HeartDisease']==1]['Age'].plot(kind='kde')

# sex
# sex male more then feamle
print(df['Sex'].value_counts())

df['Sex'].value_counts().plot(kind = 'bar')

df['Sex'].value_counts().plot(kind = 'pie',autopct='%1.1f%%')

"""#sex and HeartDisease
-> if you are male heart Disease high comparsion Feamle
 for Male 63%(1) and Feamle 25%(1)
"""

pd.crosstab(df['Sex'] , df['HeartDisease'] , normalize = 'index')*100

sns.countplot(x = df['Sex'],hue = df['HeartDisease'])

#ChestPainType
print(df['ChestPainType'].value_counts())

df['ChestPainType'].value_counts().plot(kind='bar')

df['ChestPainType'].value_counts().plot(kind='pie',autopct='%1.1f%%')

"""#ChestPainType and Heart Disease
-> for ASY(1) high risk 79%,
ATA(1) - 13.872832,
NAP(1) - 35.467980,
TA(1) - 43%
"""

sns.countplot(x = df['ChestPainType'],hue=df['HeartDisease'])

pd.crosstab(df['ChestPainType'],df['HeartDisease'],normalize='index')*100

#RestingBP
sns.histplot(df['RestingBP'])

# for RestingBP i found someone has 0 RestingBP. I Rearched  found A living person can not have 0 RestingBP.
df[df['RestingBP']==0]

df[df['RestingBP']==0].shape

df = df[df['RestingBP']!=0]

sns.histplot(df['RestingBP'])

df['RestingBP'].plot(kind='kde')

sns.boxplot(df['RestingBP'])

"""#RestingBP and HeartDisease
->if RestingBP greater then 150 - 150 then  high risk Heart  
"""

sns.barplot(x = df['HeartDisease'] , y = df['RestingBP'])

sns.violinplot(x = df['HeartDisease'] , y = df['RestingBP'])

df[df['HeartDisease']==0]['RestingBP'].plot(kind='kde')
df[df['HeartDisease']==1]['RestingBP'].plot(kind='kde')

df.head(1)

"""#Cholesterol"""

sns.histplot(df['Cholesterol'])

# I see there are many value 0. I reseached can a living human have 0 Cholesterol. No
df[df['Cholesterol']==0].shape

df['Cholesterol'].mean()

df[df['HeartDisease']==0]['Cholesterol'].plot(kind='kde')
df[df['HeartDisease']==1]['Cholesterol'].plot(kind='kde')

df['Cholesterol'].median()

df['Cholesterol'] = df['Cholesterol'].replace(0,df['Cholesterol'].median())

sns.histplot(df['Cholesterol'])

"""# Cholesterol and Heart Disease
-> Cholesterol and heart Disease lineara corr relation Cholesterol increse heart Disease slitly increse

"""

sns.barplot(x=df['HeartDisease'] , y = df['Cholesterol'])

sns.violinplot(x=df['HeartDisease'] , y = df['Cholesterol'])

df[df['HeartDisease']==0]['Cholesterol'].plot(kind='kde')
df[df['HeartDisease']==1]['Cholesterol'].plot(kind='kde')

sns.boxplot(x = df['HeartDisease'] , y = df['Cholesterol'])

sns.histplot(x = df['Cholesterol'] ,hue = df['HeartDisease'] , kde = True)

df.head(1)

"""#FastingBS"""

df['FastingBS'].value_counts()

df['FastingBS'].value_counts().plot(kind='bar')

df['FastingBS'].value_counts().plot(kind='pie',autopct ='%1.1f%%')

"""# FastingBS and heart Disease
-> if fastinBS(1) high risk heart disease 79% , if fastingBS(0) low risk heart disease 48%.
"""

sns.countplot(x = df['FastingBS'] , hue = df['HeartDisease'])

pd.crosstab(df['FastingBS'] , df['HeartDisease'] , normalize='index')*100

pd.crosstab(df['FastingBS'] , df['HeartDisease'] , normalize = 'index').plot(kind='bar' , stacked = True)

df.head(1)

"""#RestingECG"""

df['RestingECG'].value_counts()

df['RestingECG'].value_counts().plot(kind = 'bar')

df['RestingECG'].value_counts().plot(kind='pie',autopct = '%1.1f%%')

"""# RestingECG and Heart Disease
-> if Resting ECG is ST high risk (65%), LVH (56%)
"""

sns.countplot(x = df['RestingECG'],hue = df['HeartDisease'])

pd.crosstab(df['RestingECG'],df['HeartDisease'],normalize='index')*100

pd.crosstab(df['RestingECG'] , df['HeartDisease'] , normalize = 'index').plot(kind='bar' , stacked = True)

"""# MaxHR"""

sns.histplot(df['MaxHR'])

sns.displot(df['MaxHR'])

df['MaxHR'].plot(kind='kde')

sns.boxplot(df['MaxHR'])

df[df['MaxHR']<70]

"""# MaxHR and Heart Disease
-> MaxHR negtive corr related with heart disease. MaxHR low high risk heart disease.
"""

sns.barplot(data = df , x = 'HeartDisease' , y = 'MaxHR')

sns.violinplot(data = df , x = 'HeartDisease' , y = 'MaxHR')

df[df['HeartDisease']==0]['MaxHR'].plot(kind='kde')
df[df['HeartDisease']==1]['MaxHR'].plot(kind='kde')

"""# ExerciseAngina"""

df['ExerciseAngina'].value_counts()

df['ExerciseAngina'].value_counts().plot(kind='bar')

""" #ExerciseAngina and heart Disease
 -> if exerciseAngina is yes(1) then high risk heart disease  
"""

sns.countplot(x = df['ExerciseAngina'] , hue = df['HeartDisease'])

pd.crosstab(df['ExerciseAngina'] , df['HeartDisease'] , normalize = 'index').plot(kind='bar' , stacked = True)

pd.crosstab(df['ExerciseAngina'] , df['HeartDisease'] , normalize = 'index')*100

df.head(1)

"""# Oldpeak"""

sns.histplot(df['Oldpeak'])

sns.boxplot(df['Oldpeak'])

"""# Oldpeak and Heart Disease
-> if oldpeak is 0 then no risk heart disease , but as well as  oldpeak increase postive and netive then heart disease risk chance.  
"""

sns.barplot(data = df , x = 'HeartDisease' , y = 'Oldpeak')

sns.violinplot(data = df , x = 'HeartDisease' , y = 'Oldpeak')

df[df['HeartDisease']==0]['Oldpeak'].plot(kind='kde')
df[df['HeartDisease']==1]['Oldpeak'].plot(kind='kde')

"""#ST_Slope"""

df['ST_Slope'].value_counts()

df['ST_Slope'].value_counts().plot(kind='bar')

df['ST_Slope'].value_counts().plot(kind='pie',autopct= '%1.1f%%')

"""# ST_Slope and Heart  
-> if st_slope is down or float then high risk heart disease.
"""

sns.countplot(x = df['ST_Slope'] , hue = df['HeartDisease'])

pd.crosstab(df['ST_Slope'] , df['HeartDisease'] , normalize = 'index')*100

pd.crosstab(df['ST_Slope'] , df['HeartDisease'] , normalize = 'index').plot(kind='bar' , stacked = True)

"""# HeartDisease
- This data set is not imbalanced.
"""

df['HeartDisease'].value_counts()

df['HeartDisease'].value_counts().plot(kind='pie',autopct='%1.1f%%')

sns.heatmap(df.corr(numeric_only = True) , annot = True)

# spliting dataset useing trin_test_split
from sklearn.model_selection import train_test_split

X = df.drop(columns=['HeartDisease'])
y = df['HeartDisease']

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)

X_train.head(5)

X_train_encode = pd.get_dummies(X_train,drop_first=True,dtype = 'int')
X_test_encode = pd.get_dummies(X_test,drop_first=True ,dtype = 'int')

X_train_encode.head(5)

num_cols = ['Age','RestingBP','Cholesterol','MaxHR','Oldpeak']

X_train_scale = X_train_encode
X_test_scale = X_test_encode

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train_scale[num_cols] = scaler .fit_transform(X_train_scale[num_cols])
X_test_scale[num_cols] = scaler.transform(X_test_scale[num_cols])

X_train_scale.head(5)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score,confusion_matrix,classification_report
from sklearn.model_selection import GridSearchCV , cross_val_score

"""##LogisticRegression
using logisticRegression  I found recall(1) 84%
pecision(1) 90%
accuracy 85% and f1_score(1) 87%
    
"""

model = LogisticRegression()
model.fit(X_train_scale,y_train)
y_pred = model.predict(X_test_scale)

print(accuracy_score(y_test,y_pred))
print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))

scores = cross_val_score(model,X_train_scale,y_train,cv=5)
print(scores)
print(scores.mean())

"""## Hyper permater tuning using GridSearchCV for Logistic Regression
after hyper permater tuning recall 87% , precision 92% and accuracy 88%.
"""

param_grid_LR = {
    'penalty' : ['l1','l2','elasticnet'],
    'C' : [0.001,0.01,0.1,1,10,],
    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],
    'class_weight' : ['balanced',None]
}

grid = GridSearchCV(
    estimator = model,
    param_grid = param_grid_LR,
    cv = 5,
    scoring='accuracy',
    n_jobs= -1
)

grid.fit(X_train_scale,y_train)
print('Best params :',grid.best_params_)
print('Best CV score : ',grid.best_score_)

model = LogisticRegression(
    C =  1,
    class_weight = None,
    penalty = 'l1',
    solver = 'saga'
)
model.fit(X_train_scale,y_train)
y_pred = model.predict(X_test_scale)

print(accuracy_score(y_test,y_pred))
print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))

"""##Decision Tree
using Decision tree i got recall(1)-78%,precision(1)-82% and accuracy 76%  
"""

from sklearn.tree import DecisionTreeClassifier
model = DecisionTreeClassifier()
model.fit(X_train_encode,y_train)
y_pred = model.predict(X_test_encode)

print(accuracy_score(y_test,y_pred))
print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))

"""##Hyper Permater tuning using Decsion tree gredSearchCV

after tuning recall(1)-79% , precision(1)-92% and accuracy 83%
"""

param_grid_DT={
    'criterion':['gini','entropy','log_loss'],
    'max_depth':[1,2,3,4,5,6,7,8,9,10,None],
    'min_samples_split':[2,5,10,20,50],
    'min_samples_leaf':[1,2,5,10,20],
    'max_features':['auto','sqrt','log2',None],
    'class_weight':['balanced',None]
}

model_DT = DecisionTreeClassifier()

grid = GridSearchCV(
    estimator = model_DT,
    param_grid = param_grid_DT,
    cv = 5,
    scoring='accuracy',
    n_jobs= -1
)

grid.fit(X_train_scale,y_train)
print('Best params :',grid.best_params_)
print('Best CV score : ',grid.best_score_)

model = DecisionTreeClassifier(
    class_weight = None,
    criterion = 'gini',
    max_depth =  5,
    max_features = 'sqrt',
    min_samples_leaf =  5,
    min_samples_split =  20
)
model.fit(X_train_encode,y_train)
y_pred = model.predict(X_test_encode)

print(accuracy_score(y_test,y_pred))
print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))

"""##SVC(Suport vector classification)
recall(1) 87% , precision(1) 90% and acuracy 86%   
"""

from sklearn.svm  import SVC
model_svc = SVC()
model_svc.fit(X_train_scale,y_train)
y_pred = model_svc.predict(X_test_scale)

print(accuracy_score(y_test,y_pred))
print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))

"""##Hyper permater tuning using SVC  GredSearchCV
recall(1)-87% , precision(1)-90% and accuracy 86%  
"""

param_grid_svc  = {
    'kernel' : ['linear','poly','rbf','sigmoid'],
    'C' : [0.01,0.1,1,10,100],
    'gamma':['scale','auto'],
    'degree':[2,3,4],
    'class_weight':['balanced',None]
}

model_svc  = SVC()

grid = GridSearchCV(
    estimator = model_svc,
    param_grid = param_grid_svc ,
    cv = 5,
    scoring='accuracy',
    n_jobs= -1
)

grid.fit(X_train_scale,y_train)
print('Best params :',grid.best_params_)
print('Best CV score : ',grid.best_score_)

model_svc = SVC(
    C=1,
    class_weight =  None,
    gamma = 'scale',
    kernel = 'rbf'
)
model_svc.fit(X_train_encode,y_train)
y_pred = model_svc.predict(X_test_encode)

print(accuracy_score(y_test,y_pred))
print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))

"""## KNeighbors Classifier  
recall(1)-83% , precision(1)-89% and accuracy - 86%
"""

from sklearn.neighbors import KNeighborsClassifier
model_knn  = KNeighborsClassifier()
model_knn.fit(X_train_scale,y_train)
y_pred = model_knn.predict(X_test_scale)

print(accuracy_score(y_test,y_pred))
print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))

"""##Hyper permater tuning using KNN  GredSearchKNN
recall(1)-88% , precision(1)-90% and accuracy - 86%
"""

param_grid_knn = {
    'n_neighbors':[1,3,5,7,9,11,15,21],
    'weights':['uniform','distance'],
    'metric':['euclidean','manhattan','minkowski'],
}

model_knn = KNeighborsClassifier()

grid = GridSearchCV(
    estimator = model_knn,
    param_grid = param_grid_knn,
    cv = 5,
    scoring='accuracy',
    n_jobs= -1
)

grid.fit(X_train_scale,y_train)
print('Best params :',grid.best_params_)
print('Best CV score : ',grid.best_score_)

model_knn  = KNeighborsClassifier(
    metric = 'manhattan',
    n_neighbors = 11,
    weights = 'distance'
)
model_knn.fit(X_train_scale,y_train)
y_pred = model_knn.predict(X_test_scale)

print(accuracy_score(y_test,y_pred))
print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))

"""## Navi Bayes
recall(1)-79% , precision(1)-92% and accuracy - 83%
"""

from sklearn.naive_bayes import GaussianNB
model_nb = GaussianNB()
model_nb.fit(X_train_encode,y_train)
y_pred = model_nb.predict(X_test_encode )

print(accuracy_score(y_test,y_pred))
print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))

"""##Hyper permater tuning using Navie   GredSearchCV

"""

param_grid_nb  = {
    'var_smoothing':np.logspace(-12 , -1 ,20)
   }

model_nb  = GaussianNB()

grid = GridSearchCV(
    estimator = model_nb ,
    param_grid = param_grid_nb,
    cv = 5,
    scoring='accuracy',
    n_jobs= -1
)

grid.fit(X_train_scale,y_train)
print('Best params :',grid.best_params_)
print('Best CV score : ',grid.best_score_)

"""##RandomForestClassifier
recall(1)-88% , precision(1)-91% and accuracy 88%.
"""

from  sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import RandomizedSearchCV
model = RandomForestClassifier()
model.fit(X_train_encode,y_train)
y_pred = model.predict(X_test_encode)

print(accuracy_score(y_test,y_pred))
print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))

"""##Hyper permater tuning using RandomFroestClassification RandomSearchCV
##Why
- I  use RandomSearchCV instead GridSearchCV because GridSearchCV take more time
- Whenever RandomSearchCV take less time       
- when I found recall(1)-83% , precision(1)-89% and accuracy 83%.

"""

from scipy.stats import randint
param_dist_rf  ={
    'n_estimators':randint(100,200,500),
    'max_depth':[5,10,20,30,40,None],
    'min_samples_split':randint(2,5,10),
    'min_samples_leaf':randint(1,10,20),
    'max_features':['sqrt','log2',None],
    'bootstrap' : [True , False],
    'class_weight':['balanced',None]
}

from  sklearn.ensemble import RandomForestClassifier
model_rf = RandomForestClassifier()

random_search = RandomizedSearchCV(
    estimator = model_rf,
    param_distributions = param_dist_rf,
    n_iter = 50,
    cv = 5,
    scoring='accuracy',
    n_jobs= -1
)
random_search.fit(X_train_encode,y_train)
print('Best params :',random_search.best_params_)
print(random_search.best_score_ )

model_rf  = RandomForestClassifier(
    bootstrap = True,
    class_weight = 'balanced',
    max_depth = 10,
    max_features = 'log2',
    min_samples_leaf = 21,
    min_samples_split = 14,
    n_estimators = 639
)
model_rf.fit(X_train_encode,y_train)
y_pred = model_rf.predict(X_test_encode)

print(accuracy_score(y_test,y_pred))
print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))

"""##AdaBoostClassifier
- recall(1)-87% , precision(1)-91% and acuracy 86%.
"""

from sklearn.ensemble import AdaBoostClassifier
model_AB = AdaBoostClassifier()

model_AB.fit(X_train_encode,y_train)
y_pred = model_AB.predict(X_test_encode)

print(accuracy_score(y_test,y_pred))
print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))

"""##Hyper permater tuning using AdaBoostClassifier GredSearchCV
-  recall(1)-85% , precision(1)-91% and accruacy 86%.
"""

parm_grid_ad = {
    'n_estimators':[50,100,200,500],
    'learning_rate':[0.01,0.5,1,10],
    'estimator' : [
                   DecisionTreeClassifier(max_depth=1),
                   DecisionTreeClassifier(max_depth=2),
                   DecisionTreeClassifier(max_depth=3),
    ]
}

grid = GridSearchCV(
    estimator = model_AB,
    param_grid = parm_grid_ad,
    cv = 5,
    scoring='accuracy',
    n_jobs= -1
)
grid.fit(X_train_scale,y_train)
print('Best params :',grid.best_params_)
print('Best CV score : ',grid.best_score_)

model_AB = AdaBoostClassifier(
    estimator = DecisionTreeClassifier(max_depth=1),
    learning_rate = 0.5,
    n_estimators = 200,

)

model_AB.fit(X_train_encode,y_train)
y_pred = model_AB.predict(X_test_encode)

print(accuracy_score(y_test,y_pred))
print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))

"""##GradientBoostingClassifier
- recall(1)-91% , precision(1)-91% and accruacy 89%.
"""

from sklearn.ensemble import GradientBoostingClassifier

model_GD  = GradientBoostingClassifier()

model_GD.fit(X_train_encode,y_train)
y_pred = model_GD.predict(X_test_encode);

print(accuracy_score(y_test,y_pred))
print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))

"""##Hyper permater tuning using GradientBoostingClassifier GredSearchCV
- recall(1)-90% , precision(1)-90 and accuracy 88%
"""

from scipy.stats import randint
param_grid_GD = {
    'n_estimators':randint(100,200,300),
    'learning_rate':[0.01,0.5,1,10],
    'max_depth':[2,3,4,5],
    'min_samples_split':[2,5,10],
    'min_samples_leaf':[1,2,4],
    'max_features':['sqrt','log2',None]
}

from sklearn.model_selection import RandomizedSearchCV
random_search = RandomizedSearchCV(
    estimator = model_GD,
    param_distributions = param_grid_GD,
    n_iter = 50,
    cv = 5,
    scoring='accuracy',
    n_jobs= -1
)
random_search.fit(X_train_encode,y_train)
print('Best params :',random_search.best_params_)
print(random_search.best_score_ )

model_GD  = GradientBoostingClassifier(
    learning_rate = 0.01,
    max_depth = 5,
    max_features = 'sqrt',
    min_samples_leaf = 2,
    min_samples_split =2,
    n_estimators = 424
)

model_GD.fit(X_train_encode,y_train)
y_pred = model_GD.predict(X_test_encode);

print(accuracy_score(y_test,y_pred))
print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))

"""##XGBClassifier"""

from xgboost import XGBClassifier
model_xgb = XGBClassifier()

model_xgb.fit(X_train_encode,y_train)
y_pred = model_xgb.predict(X_test_encode)

print(accuracy_score(y_test,y_pred))
print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))

"""##Hyper permater tuning using XGBClassifier RandomSearchCV
- recall(1)-92% , precision(1)-89% and accuracy 89%.
"""

param_grid_xgb = {
    'n_estimators':[100,200,300,500],
    'learning_rate':[0.01,0.05,0.1,0.2],
    'max_depth':[2,3,4,5],
    'min_child_weight':[1,2,3,4],
    'subsample':[0.6,0.8,1.0],
    'gamma' : [0,0.1,0.2,0.5],
    'reg_alpha':[0,0.1,0.2,0.5],
    'reg_lambda':[1,1.5,2,3]
}

random_search = RandomizedSearchCV(
    estimator = model_xgb,
    param_distributions = param_grid_xgb,
    n_iter = 50,
    cv = 5,
    scoring='accuracy',
    n_jobs= -1
)
random_search.fit(X_train_encode,y_train)
print('Best params :',random_search.best_params_)
print(random_search.best_score_ )

model_xgb = XGBClassifier(
    subsample =  1.0,
    reg_lambda =3,
    reg_alpha =0.5,
    n_estimators= 200,
    min_child_weight=2,
    max_depth=4,
    learning_rate=0.05,
    gamma=0.1

)

model_xgb.fit(X_train_encode,y_train)
y_pred = model_xgb.predict(X_test_encode)

print(accuracy_score(y_test,y_pred))
print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))

"""## i am going to do again using ml"""

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder,OrdinalEncoder
from sklearn.pipeline import Pipeline , make_pipeline
from xgboost import XGBClassifier

X_train.head(1)

# In my dataset there are no missing value so i will start my pipe direct encoder
trf1 = ColumnTransformer([
    ('ohe',OneHotEncoder(drop='first',handle_unknown ='ignore'),[1,2,6,8]),
    ('oe',OrdinalEncoder(categories=[['Up','Flat','Down']]),[10])
    ],remainder='passthrough')

# trian xgboost model becasue this model gave me best result for recall , precision , f1_score and accuracy
trf2 = XGBClassifier(
    subsample =  1.0,
    reg_lambda =3,
    reg_alpha =0.5,
    n_estimators= 200,
    min_child_weight=2,
    max_depth=4,
    learning_rate=0.05,
    gamma=0.1
)

"""##Create Pipeline
- create Pipeline using pipeline class
- i can also use make_pipeline class
- both are same a small defferect  
"""

pipe = Pipeline([
    ('trf1',trf1),
    ('trf2',trf2)
])

pipe.fit(X_train,y_train)

y_pred = pipe.predict(X_test)

print(accuracy_score(y_test,y_pred))
print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))

from sklearn.model_selection import cross_val_score
cross_val_score(pipe,X_train,y_train,cv = 5 , scoring = 'f1').mean()

import pickle
pickle.dump(pipe,open('new_pipe.pkl','wb'))

